{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aghyaddeeb/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "CONFIG.API.APIKEY = os.getenv(\"NDIF_KEY\")\n",
    "# model_id, remote, device = \"meta-llama/Llama-3.1-70B\", True, \"auto\"\n",
    "# model_id, remote, device = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\", True, \"auto\"\n",
    "model_id, remote, device = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", True, \"auto\"\n",
    "# model_id, remote, device = \"HuggingFaceTB/SmolLM2-135M-Instruct\", False, \"mps\"\n",
    "model = LanguageModel(model_id, device_map=device, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'think': [128013], 'end_of_think': [128014]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_special_tokens():\n",
    "    # temp = [{\"role\": \"assistant\", \"content\": \"\"}, {\"role\": \"user\", \"content\": \"\"}, {\"role\"}]\n",
    "    # assistant = model.tokenizer.apply_chat_template(temp, tokenize=False, add_generation_prompt=True)\n",
    "    # print(assistant)\n",
    "    think = model.tokenizer(\"<think>\", add_special_tokens=False)[\"input_ids\"]\n",
    "    end_of_think = model.tokenizer(\"</think>\", add_special_tokens=False)[\"input_ids\"]\n",
    "    return dict(think=think, end_of_think=end_of_think)\n",
    "\n",
    "get_special_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 18 (2169851429.py, line 22)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprompt = \"hi\"\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'with' statement on line 18\n"
     ]
    }
   ],
   "source": [
    "# think_token, end_of_think_token = get_special_tokens()\n",
    "think_token = get_special_tokens()[\"think\"]\n",
    "end_of_think_token = get_special_tokens()[\"end_of_think\"]\n",
    "# print(end_of_think_token)\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, stop_token_ids):\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        for stop_id in self.stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_token_ids=[end_of_think_token])])\n",
    "\n",
    "with model.generate(\"hi\", remote=remote, stopping_criteria=stopping_criteria, max_new_tokens=1000):\n",
    "# with model.generate(\"hi\", remote=remote, stopping_criteria=stopping_criteria):\n",
    "# with model.generate(\"hi\", remote=remote, stopping_criteria=stopping_criteria):\n",
    "# with model.generate(\"hi\", remote=remote, max_new_tokens=1000):\n",
    "prompt = \"hi\"\n",
    "temp = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "input = model.tokenizer.apply_chat_template(temp, tokenize=True, add_generation_prompt=True)\n",
    "with model.generate(input, remote=remote, eos_token_id=end_of_think_token, max_new_tokens=1000):\n",
    "    out =  model.generator.output.save()\n",
    "model.tokenizer.decode(out[0])\n",
    "# out\n",
    "# out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 116) (3755689052.py, line 116)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"{max_prob_token_idx=}\"\")\u001b[39m\n                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 116)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def find_first_token_index(tensor, value) -> int:\n",
    "    \"\"\"Find the first index where tensor equals value.\"\"\"\n",
    "    if isinstance(value, list):\n",
    "        value = torch.tensor(value)\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        tensor = torch.tensor(tensor).view((-1))\n",
    "    print(f\"{tensor=}{value=}\")\n",
    "    indices = (tensor == value).nonzero()\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        return indices[0].item()\n",
    "    return None\n",
    "\n",
    "def find_last_token_index(tensor, value) -> int:\n",
    "    # assert isinstance(tensor, torch.Tensor), f\"{tensor=}\"\n",
    "    if isinstance(value, list):\n",
    "        value = torch.tensor(value)\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        tensor = torch.tensor(tensor).view((-1))\n",
    "    print(f\"{tensor=}{value=}\")\n",
    "    indices = (tensor == value).nonzero()\n",
    "    # print(f\"{indices=}\")\n",
    "    if len(indices) > 0:\n",
    "        return indices[-1].item()\n",
    "    return None\n",
    "\n",
    "def find_length_between_tokens(tensor, start_token, end_token) -> int:\n",
    "    end = find_last_token_index(tensor, end_token)\n",
    "    start = find_first_token_index(tensor, start_token)\n",
    "    print(f\"{end=} {start=}\")\n",
    "    return end - start\n",
    "    \n",
    "def get_logits(prompt):\n",
    "    print(f\"prompt from get_logits_after_thinking: {prompt=}\")\n",
    "    temp = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    input = model.tokenizer.apply_chat_template(temp, tokenize=True, add_generation_prompt=True)\n",
    "    print(f\"{model.tokenizer.decode(input)=}\")\n",
    "    with model.trace(input, remote=remote):\n",
    "        out = model.output.save()\n",
    "    logits = out[\"logits\"][0]\n",
    "    return logits\n",
    "\n",
    "def generate_till_eos(prompt, max_new_tokens=1000):\n",
    "    temp = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    input = model.tokenizer.apply_chat_template(temp, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    # input = model.tokenizer.apply_chat_template(temp, tokenize=False, add_generation_prompt=True)\n",
    "    # print(f\"{model.tokenizer.decode(input)=}\")\n",
    "    # display(f\"{input=}\")\n",
    "    # display(f\"{input.shape=}\")\n",
    "    # while True:\n",
    "    #     with model.trace(input, remote=remote):\n",
    "    #         out = model.output.save()\n",
    "    #     display(f\"{out.shape=}\")\n",
    "    #     logits = out[\"logits\"][0, -1]\n",
    "    #     new_token = logits.argmax()\n",
    "    #     print(f\"{new_token.shape}\")\n",
    "    #     print(f\"{torch.unsqueeze(new_token, 0).shape}\")\n",
    "    #     input = torch.cat((input, torch.unsqueeze(torch.unsqueeze(new_token, 0), 0)), dim=-1)\n",
    "    #     if new_token == end_of_think:\n",
    "    #         break\n",
    "    with model.generate(input, remote=remote, max_new_tokens=max_new_tokens):\n",
    "    # with model.generate(input, remote=remote, max_new_tokens=max_new_tokens):\n",
    "        prompt_with_thinking = model.generator.output.save()\n",
    "\n",
    "    return prompt_with_thinking\n",
    "\n",
    "def get_logits_after_thinking(prompt, max_new_tokens=1000, get_full_generation=False):\n",
    "    special_tokens = get_special_tokens()\n",
    "    think_token = special_tokens[\"think\"]\n",
    "    end_of_think_token = special_tokens[\"end_of_think\"]\n",
    "    print(f\"prompt from get_logits: {prompt=}\")\n",
    "    temp = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    input = model.tokenizer.apply_chat_template(temp, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    # input = model.tokenizer.apply_chat_template(temp, tokenize=False, add_generation_prompt=True)\n",
    "    # print(f\"{model.tokenizer.decode(input)=}\")\n",
    "    # display(f\"{input=}\")\n",
    "    # display(f\"{input.shape=}\")\n",
    "    # while True:\n",
    "    #     with model.trace(input, remote=remote):\n",
    "    #         out = model.output.save()\n",
    "    #     display(f\"{out.shape=}\")\n",
    "    #     logits = out[\"logits\"][0, -1]\n",
    "    #     new_token = logits.argmax()\n",
    "    #     print(f\"{new_token.shape}\")\n",
    "    #     print(f\"{torch.unsqueeze(new_token, 0).shape}\")\n",
    "    #     input = torch.cat((input, torch.unsqueeze(torch.unsqueeze(new_token, 0), 0)), dim=-1)\n",
    "    #     if new_token == end_of_think:\n",
    "    #         break\n",
    "    with model.generate(input, remote=remote, eos_token_id=end_of_think_token, max_new_tokens=max_new_tokens):\n",
    "    # with model.generate(input, remote=remote, max_new_tokens=max_new_tokens):\n",
    "        prompt_with_thinking = model.generator.output.save()\n",
    "\n",
    "    thinking_length = find_length_between_tokens(\n",
    "        prompt_with_thinking, think_token, end_of_think_token\n",
    "    )\n",
    "    \n",
    "\n",
    "    wanted_tokens = [32, 33, 34, 35]\n",
    "    def get_one():\n",
    "        with model.trace(prompt_with_thinking, remote=remote):\n",
    "            logits = model.output[0][-1][-1][wanted_tokens].save()\n",
    "        return logits\n",
    "    # Get the logits for the specified tokens\n",
    "    logits = get_one()\n",
    "    \n",
    "    # Get the token with the maximum probability\n",
    "    max_prob_token_idx = logits.argmax(dim=-1).item()\n",
    "    while max_prob_token_idx not in wanted_tokens:\n",
    "        print(f\"{max_prob_token_idx=}\"\")\n",
    "        print(f\"{prompt_with_thinking.shape=}\")\n",
    "        print(f\"{logits=}\")\n",
    "        prompt_with_thinking = torch.cat(prompt_with_thinking, )\n",
    "        logits = get_one()\n",
    "        max_prob_token_idx = logits.argmax().item()\n",
    "\n",
    "    full = []\n",
    "    if get_full_generation:\n",
    "        with model.generate(prompt_with_thinking, remote=remote, max_new_tokens=max_new_tokens):\n",
    "        # with model.generate(input, remote=remote, max_new_tokens=max_new_tokens):\n",
    "            full = model.generator.output.save()\n",
    "\n",
    "    # Convert to a tensor with the same dtype as the original logits\n",
    "    logits = logits.to(logits.dtype)\n",
    "\n",
    "    display(f\"{logits.shape=}\")\n",
    "    return logits, prompt_with_thinking, thinking_length, full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt from get_logits: prompt='Answer only with A, B, C or D. Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\nA) 0\\nB) 4\\nC) 2\\nD) 6\\nAfter thinking about the question, output only one of the letters A, B, C, D. There should be nothing between the </think> token and the letter you output'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 16:05:55,598 3b35a50a-d8b6-4976-806e-088d028f31cf - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-27 16:05:56,110 3b35a50a-d8b6-4976-806e-088d028f31cf - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-27 16:05:56,623 3b35a50a-d8b6-4976-806e-088d028f31cf - RUNNING: Your job has started running.\n",
      "2025-03-27 16:06:22,124 3b35a50a-d8b6-4976-806e-088d028f31cf - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 7.52k/7.52k [00:00<00:00, 12.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor=tensor([128000, 128011,  16533,   1193,    449,    362,     11,    426,     11,\n",
      "           356,    477,    423,     13,  16225,     25,   7531,    279,   8547,\n",
      "           369,    279,   2728,   2115,   9070,   1229,  84173,      7,     17,\n",
      "           705,  18430,      7,     18,    705,  18430,      7,    972,    595,\n",
      "           927,   1229,    627,     32,      8,    220,     15,    198,     33,\n",
      "             8,    220,     19,    198,     34,      8,    220,     17,    198,\n",
      "            35,      8,    220,     21,    198,   6153,   7422,    922,    279,\n",
      "          3488,     11,   2612,   1193,    832,    315,    279,  12197,    362,\n",
      "            11,    426,     11,    356,     11,    423,     13,   2684,   1288,\n",
      "           387,   4400,   1990,    279,    220, 128014,   4037,    323,    279,\n",
      "          6661,    499,   2612, 128012, 128013,    198,  72586,     11,    779,\n",
      "           358,    617,    420,   3488,   1618,     25,   7531,    279,   8547,\n",
      "           315,    279,   2115,   9070,   1229,  84173,      7,     17,    705,\n",
      "         18430,      7,     18,    705,  18430,      7,    972,    595,    927,\n",
      "          1229,     13,    578,   2671,    527,    362,      8,    220,     15,\n",
      "            11,    426,      8,    220,     19,     11,    356,      8,    220,\n",
      "            17,     11,    423,      8,    220,     21,     13,  89290,     11,\n",
      "         17339,     11,   1095,    757,   1456,    311,   7216,    420,    704,\n",
      "          3094,    555,   3094,    382,   5451,   1022,     11,    358,   6227,\n",
      "           430,    994,  14892,    449,   2115,  20300,     11,    279,   8547,\n",
      "           315,    279,   9070,    374,    279,   8547,    315,    279,  17832,\n",
      "         48411,    315,    279,  14143,    927,    279,   2385,   2115,     13,\n",
      "          2100,     11,    304,    420,   1162,     11,    279,   2385,   2115,\n",
      "           374,   1229,     11,    323,    279,   9070,    374,   1229,  84173,\n",
      "             7,     17,    705,  18430,      7,     18,    705,  18430,      7,\n",
      "           972,   4682,    358,   1205,    311,   1505,    279,  17832,  48411,\n",
      "          6832,  20282,    527,   1521,   5540,    323,   8417,   1202,   8547,\n",
      "           382,   4071,   1603,    430,     11,   7344,    358,   1288,   1817,\n",
      "           422,   1521,   5540,    527,   2736,    304,    264,   9333,   9070,\n",
      "           477,    422,    814,    649,    387,  13605,    304,   3878,    315,\n",
      "          3885,     13,   1789,   2937,     11,  18430,      7,    972,      8,\n",
      "           649,    387,  44899,   1606,    220,    972,    374,    220,     24,\n",
      "             9,     17,     11,    323,    220,     24,    374,    264,   4832,\n",
      "          9518,     13,   2100,  18430,      7,    972,      8,    374,  18430,\n",
      "             7,     24,      9,     17,      8,    902,    374,    220,     18,\n",
      "             9,  27986,      7,     17,    570,   3011,   3445,  18430,      7,\n",
      "           972,      8,    374,   1120,    220,     18,   3115,  18430,      7,\n",
      "            17,    570,   2100,     11,  18430,      7,    972,      8,    374,\n",
      "          2736,    304,   1229,  84173,      7,     17,   4682,  15636,     11,\n",
      "           279,   2115,   1229,  84173,      7,     17,    705,  18430,      7,\n",
      "            18,    705,  18430,      7,    972,    595,    649,    387,  59624,\n",
      "           439,   1229,  84173,      7,     17,    705,  18430,      7,     18,\n",
      "           705,    220,     18,      9,  27986,      7,     17,    595,    382,\n",
      "          4071,   2533,    220,     18,      9,  27986,      7,     17,      8,\n",
      "           374,   2736,    304,   1229,  84173,      7,     17,   5850,   7999,\n",
      "           433,   3250,    956,    923,   4205,    502,    311,    279,   2115,\n",
      "            13,   2100,     11,    279,   2115,   1229,  84173,      7,     17,\n",
      "           705,  18430,      7,     18,    705,  18430,      7,    972,    595,\n",
      "           374,   3604,   6273,    311,   1229,  84173,      7,     17,    705,\n",
      "         18430,      7,     18,   4682,   3011,  15858,   9803,   2574,   1606,\n",
      "          1457,    358,   1120,   1205,    311,   1505,    279,   8547,    315,\n",
      "          1229,  84173,      7,     17,    705,  18430,      7,     18,    595,\n",
      "           927,   1229,    382,     40,   1440,    430,   1229,  84173,      7,\n",
      "            17,    705,  18430,      7,     18,    595,    374,    264,  28814,\n",
      "          2115,     11,   7438,    433,    596,    279,   2115,   8066,    555,\n",
      "          2225,  18430,      7,     17,      8,    323,  18430,      7,     18,\n",
      "           570,    578,   8547,    315,    264,  28814,   2115,    374,    279,\n",
      "          2027,    315,    279,  12628,    315,    279,   3927,  20300,    422,\n",
      "           814,    527,  13790,    398,  85884,     13,   2100,     11,   1095,\n",
      "           596,   1817,    430,    382,   5451,     11,    279,   8547,    315,\n",
      "          1229,  84173,      7,     17,    595,    927,   1229,    374,    220,\n",
      "            17,   1606,    279,  17832,  48411,    374,    865,     61,     17,\n",
      "           482,    220,     17,     13,  35339,     11,    279,   8547,    315,\n",
      "          1229,  84173,      7,     18,    595,    927,   1229,    374,   1101,\n",
      "           220,     17,    449,  17832,  48411,    865,     61,     17,    482,\n",
      "           220,     18,     13,   4800,     11,    527,   1521,  20300,  85884,\n",
      "            30,   3011,    374,     11,    656,    814,   4430,    904,   4279,\n",
      "          5540,   1980,  11649,     11,   2533,  18430,      7,     17,      8,\n",
      "           323,  18430,      7,     18,      8,    527,   2225,  61754,    323,\n",
      "           872,  32440,    527,    220,     17,    323,    220,     18,     11,\n",
      "           902,    527,  12742,  50533,     11,   1070,    596,    912,  28347,\n",
      "          1990,   1124,     13,   2100,     11,    872,  20300,    527,  13790,\n",
      "           398,  85884,     13,  15636,     11,    279,   8547,    315,   1229,\n",
      "         84173,      7,     17,    705,  18430,      7,     18,    595,    927,\n",
      "          1229,   1288,    387,    220,     17,    353,    220,     17,    284,\n",
      "           220,     19,     13,   2100,     11,    279,   8547,    315,    279,\n",
      "          9070,    374,    220,     19,    382,  14524,     11,    719,   1095,\n",
      "           757,   2033,  16313,     13,   2209,   1070,    904,   6140,    430,\n",
      "         18430,      7,    972,      8,   1436,    923,   2555,    775,     30,\n",
      "          8489,     11,    439,    358,   3463,   6931,     11,  18430,      7,\n",
      "           972,      8,    374,   1120,    220,     18,      9,  27986,      7,\n",
      "            17,    705,    902,    374,   2736,    304,   1229,  84173,      7,\n",
      "            17,   4682,   2100,     11,   7999,  18430,      7,    972,      8,\n",
      "          3250,    956,    923,    904,    502,   5540,    311,    279,   2115,\n",
      "            13,   2100,     11,    279,   2115,    374,  13118,   1229,  84173,\n",
      "             7,     17,    705,  18430,      7,     18,   5850,    902,    706,\n",
      "          8547,    220,     19,    382,  55915,     11,    279,   4495,   4320,\n",
      "          1288,    387,    426,      8,    220,     19,    627, 128014])value=tensor([128014])\n",
      "tensor=tensor([128000, 128011,  16533,   1193,    449,    362,     11,    426,     11,\n",
      "           356,    477,    423,     13,  16225,     25,   7531,    279,   8547,\n",
      "           369,    279,   2728,   2115,   9070,   1229,  84173,      7,     17,\n",
      "           705,  18430,      7,     18,    705,  18430,      7,    972,    595,\n",
      "           927,   1229,    627,     32,      8,    220,     15,    198,     33,\n",
      "             8,    220,     19,    198,     34,      8,    220,     17,    198,\n",
      "            35,      8,    220,     21,    198,   6153,   7422,    922,    279,\n",
      "          3488,     11,   2612,   1193,    832,    315,    279,  12197,    362,\n",
      "            11,    426,     11,    356,     11,    423,     13,   2684,   1288,\n",
      "           387,   4400,   1990,    279,    220, 128014,   4037,    323,    279,\n",
      "          6661,    499,   2612, 128012, 128013,    198,  72586,     11,    779,\n",
      "           358,    617,    420,   3488,   1618,     25,   7531,    279,   8547,\n",
      "           315,    279,   2115,   9070,   1229,  84173,      7,     17,    705,\n",
      "         18430,      7,     18,    705,  18430,      7,    972,    595,    927,\n",
      "          1229,     13,    578,   2671,    527,    362,      8,    220,     15,\n",
      "            11,    426,      8,    220,     19,     11,    356,      8,    220,\n",
      "            17,     11,    423,      8,    220,     21,     13,  89290,     11,\n",
      "         17339,     11,   1095,    757,   1456,    311,   7216,    420,    704,\n",
      "          3094,    555,   3094,    382,   5451,   1022,     11,    358,   6227,\n",
      "           430,    994,  14892,    449,   2115,  20300,     11,    279,   8547,\n",
      "           315,    279,   9070,    374,    279,   8547,    315,    279,  17832,\n",
      "         48411,    315,    279,  14143,    927,    279,   2385,   2115,     13,\n",
      "          2100,     11,    304,    420,   1162,     11,    279,   2385,   2115,\n",
      "           374,   1229,     11,    323,    279,   9070,    374,   1229,  84173,\n",
      "             7,     17,    705,  18430,      7,     18,    705,  18430,      7,\n",
      "           972,   4682,    358,   1205,    311,   1505,    279,  17832,  48411,\n",
      "          6832,  20282,    527,   1521,   5540,    323,   8417,   1202,   8547,\n",
      "           382,   4071,   1603,    430,     11,   7344,    358,   1288,   1817,\n",
      "           422,   1521,   5540,    527,   2736,    304,    264,   9333,   9070,\n",
      "           477,    422,    814,    649,    387,  13605,    304,   3878,    315,\n",
      "          3885,     13,   1789,   2937,     11,  18430,      7,    972,      8,\n",
      "           649,    387,  44899,   1606,    220,    972,    374,    220,     24,\n",
      "             9,     17,     11,    323,    220,     24,    374,    264,   4832,\n",
      "          9518,     13,   2100,  18430,      7,    972,      8,    374,  18430,\n",
      "             7,     24,      9,     17,      8,    902,    374,    220,     18,\n",
      "             9,  27986,      7,     17,    570,   3011,   3445,  18430,      7,\n",
      "           972,      8,    374,   1120,    220,     18,   3115,  18430,      7,\n",
      "            17,    570,   2100,     11,  18430,      7,    972,      8,    374,\n",
      "          2736,    304,   1229,  84173,      7,     17,   4682,  15636,     11,\n",
      "           279,   2115,   1229,  84173,      7,     17,    705,  18430,      7,\n",
      "            18,    705,  18430,      7,    972,    595,    649,    387,  59624,\n",
      "           439,   1229,  84173,      7,     17,    705,  18430,      7,     18,\n",
      "           705,    220,     18,      9,  27986,      7,     17,    595,    382,\n",
      "          4071,   2533,    220,     18,      9,  27986,      7,     17,      8,\n",
      "           374,   2736,    304,   1229,  84173,      7,     17,   5850,   7999,\n",
      "           433,   3250,    956,    923,   4205,    502,    311,    279,   2115,\n",
      "            13,   2100,     11,    279,   2115,   1229,  84173,      7,     17,\n",
      "           705,  18430,      7,     18,    705,  18430,      7,    972,    595,\n",
      "           374,   3604,   6273,    311,   1229,  84173,      7,     17,    705,\n",
      "         18430,      7,     18,   4682,   3011,  15858,   9803,   2574,   1606,\n",
      "          1457,    358,   1120,   1205,    311,   1505,    279,   8547,    315,\n",
      "          1229,  84173,      7,     17,    705,  18430,      7,     18,    595,\n",
      "           927,   1229,    382,     40,   1440,    430,   1229,  84173,      7,\n",
      "            17,    705,  18430,      7,     18,    595,    374,    264,  28814,\n",
      "          2115,     11,   7438,    433,    596,    279,   2115,   8066,    555,\n",
      "          2225,  18430,      7,     17,      8,    323,  18430,      7,     18,\n",
      "           570,    578,   8547,    315,    264,  28814,   2115,    374,    279,\n",
      "          2027,    315,    279,  12628,    315,    279,   3927,  20300,    422,\n",
      "           814,    527,  13790,    398,  85884,     13,   2100,     11,   1095,\n",
      "           596,   1817,    430,    382,   5451,     11,    279,   8547,    315,\n",
      "          1229,  84173,      7,     17,    595,    927,   1229,    374,    220,\n",
      "            17,   1606,    279,  17832,  48411,    374,    865,     61,     17,\n",
      "           482,    220,     17,     13,  35339,     11,    279,   8547,    315,\n",
      "          1229,  84173,      7,     18,    595,    927,   1229,    374,   1101,\n",
      "           220,     17,    449,  17832,  48411,    865,     61,     17,    482,\n",
      "           220,     18,     13,   4800,     11,    527,   1521,  20300,  85884,\n",
      "            30,   3011,    374,     11,    656,    814,   4430,    904,   4279,\n",
      "          5540,   1980,  11649,     11,   2533,  18430,      7,     17,      8,\n",
      "           323,  18430,      7,     18,      8,    527,   2225,  61754,    323,\n",
      "           872,  32440,    527,    220,     17,    323,    220,     18,     11,\n",
      "           902,    527,  12742,  50533,     11,   1070,    596,    912,  28347,\n",
      "          1990,   1124,     13,   2100,     11,    872,  20300,    527,  13790,\n",
      "           398,  85884,     13,  15636,     11,    279,   8547,    315,   1229,\n",
      "         84173,      7,     17,    705,  18430,      7,     18,    595,    927,\n",
      "          1229,   1288,    387,    220,     17,    353,    220,     17,    284,\n",
      "           220,     19,     13,   2100,     11,    279,   8547,    315,    279,\n",
      "          9070,    374,    220,     19,    382,  14524,     11,    719,   1095,\n",
      "           757,   2033,  16313,     13,   2209,   1070,    904,   6140,    430,\n",
      "         18430,      7,    972,      8,   1436,    923,   2555,    775,     30,\n",
      "          8489,     11,    439,    358,   3463,   6931,     11,  18430,      7,\n",
      "           972,      8,    374,   1120,    220,     18,      9,  27986,      7,\n",
      "            17,    705,    902,    374,   2736,    304,   1229,  84173,      7,\n",
      "            17,   4682,   2100,     11,   7999,  18430,      7,    972,      8,\n",
      "          3250,    956,    923,    904,    502,   5540,    311,    279,   2115,\n",
      "            13,   2100,     11,    279,   2115,    374,  13118,   1229,  84173,\n",
      "             7,     17,    705,  18430,      7,     18,   5850,    902,    706,\n",
      "          8547,    220,     19,    382,  55915,     11,    279,   4495,   4320,\n",
      "          1288,    387,    426,      8,    220,     19,    627, 128014])value=tensor([128013])\n",
      "end=790 start=94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 16:06:25,921 ae4bbb38-5016-49e2-aac7-5472e9cc19b6 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-27 16:06:26,727 ae4bbb38-5016-49e2-aac7-5472e9cc19b6 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-27 16:06:28,031 ae4bbb38-5016-49e2-aac7-5472e9cc19b6 - RUNNING: Your job has started running.\n",
      "2025-03-27 16:06:30,107 ae4bbb38-5016-49e2-aac7-5472e9cc19b6 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.24k/1.24k [00:00<00:00, 4.96MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 16:06:34,748 7d572889-781c-49c1-9a79-d94c011938d4 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-27 16:06:36,659 7d572889-781c-49c1-9a79-d94c011938d4 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-27 16:06:37,787 7d572889-781c-49c1-9a79-d94c011938d4 - RUNNING: Your job has started running.\n",
      "2025-03-27 16:06:39,320 7d572889-781c-49c1-9a79-d94c011938d4 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.24k/1.24k [00:00<00:00, 6.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 16:06:44,647 02e8a4ad-0ad7-4ee7-add6-161d1759c988 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-27 16:06:45,364 02e8a4ad-0ad7-4ee7-add6-161d1759c988 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-27 16:06:45,672 02e8a4ad-0ad7-4ee7-add6-161d1759c988 - RUNNING: Your job has started running.\n",
      "2025-03-27 16:06:46,491 02e8a4ad-0ad7-4ee7-add6-161d1759c988 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.24k/1.24k [00:00<00:00, 4.88MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 16:06:47,310 37b88e95-ba5b-4449-9e28-17fcc3395e14 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-27 16:06:47,617 37b88e95-ba5b-4449-9e28-17fcc3395e14 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-27 16:06:48,334 37b88e95-ba5b-4449-9e28-17fcc3395e14 - RUNNING: Your job has started running.\n",
      "2025-03-27 16:06:48,495 37b88e95-ba5b-4449-9e28-17fcc3395e14 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.24k/1.24k [00:00<00:00, 3.33MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 16:06:49,459 bf930e1e-7b49-45d5-a0c0-8cd31d64d9a4 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-27 16:06:50,074 bf930e1e-7b49-45d5-a0c0-8cd31d64d9a4 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-27 16:06:50,381 bf930e1e-7b49-45d5-a0c0-8cd31d64d9a4 - RUNNING: Your job has started running.\n",
      "2025-03-27 16:06:50,560 bf930e1e-7b49-45d5-a0c0-8cd31d64d9a4 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.24k/1.24k [00:00<00:00, 7.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 16:06:51,507 9e21bfca-1dc8-4814-88fb-b65b4bbd370e - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-27 16:06:52,021 9e21bfca-1dc8-4814-88fb-b65b4bbd370e - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-27 16:06:52,327 9e21bfca-1dc8-4814-88fb-b65b4bbd370e - RUNNING: Your job has started running.\n",
      "2025-03-27 16:06:52,654 9e21bfca-1dc8-4814-88fb-b65b4bbd370e - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.24k/1.24k [00:00<00:00, 5.65MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 16:06:53,556 75e69b6a-4fc4-4828-9515-024e17d419f6 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-27 16:06:53,863 75e69b6a-4fc4-4828-9515-024e17d419f6 - APPROVED: Your job was approved and is waiting to be run.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output = \u001b[43mget_logits_after_thinking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model.tokenizer.decode(output[\u001b[32m1\u001b[39m].view((-\u001b[32m1\u001b[39m)))\n\u001b[32m      3\u001b[39m model.tokenizer.decode(output[\u001b[32m0\u001b[39m].view((-\u001b[32m1\u001b[39m)).argmax())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mget_logits_after_thinking\u001b[39m\u001b[34m(prompt, max_new_tokens)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m max_prob_token_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m wanted_tokens:\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mprint\u001b[39m(max_prob_token_idx)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     logits = \u001b[43mget_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     max_prob_token_idx = logits.argmax().item()\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Convert to a tensor with the same dtype as the original logits\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mget_logits_after_thinking.<locals>.get_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_one\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_with_thinking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwanted_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[39m, in \u001b[36mInterleavingTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m.invoker.\u001b[34m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m._model._envoy._reset()\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[39m, in \u001b[36mTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[32m     23\u001b[39m GlobalTracingContext.try_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/tracing/contexts/base.py:82\u001b[39m, in \u001b[36mContext.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     78\u001b[39m graph = graph.stack.pop()\n\u001b[32m     80\u001b[39m graph.alive = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/intervention/backends/remote.py:77\u001b[39m, in \u001b[36mRemoteBackend.__call__\u001b[39m\u001b[34m(self, graph)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph: Graph):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocking:\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# Do blocking request.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocking_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m \n\u001b[32m     81\u001b[39m         \u001b[38;5;66;03m# Otherwise we are getting the status / result of the existing job.\u001b[39;00m\n\u001b[32m     82\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.non_blocking_request(graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/intervention/backends/remote.py:305\u001b[39m, in \u001b[36mRemoteBackend.blocking_request\u001b[39m\u001b[34m(self, graph)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;66;03m# Loop until\u001b[39;00m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    303\u001b[39m \n\u001b[32m    304\u001b[39m         \u001b[38;5;66;03m# Get pickled bytes value from the websocket.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m         response = \u001b[43msio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m    306\u001b[39m         \u001b[38;5;66;03m# Convert to pydantic object.\u001b[39;00m\n\u001b[32m    307\u001b[39m         response = ResponseModel.unpickle(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/socketio/simple_client.py:175\u001b[39m, in \u001b[36mSimpleClient.receive\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connected:\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DisconnectedError()\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.input_event.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/threading.py:634\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    632\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/threading.py:334\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer only with A, B, C or D. Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
    "A) 0\n",
    "B) 4\n",
    "C) 2\n",
    "D) 6\n",
    "After thinking about the question, output only one of the letters A, B, C, D. There should be nothing between the </think> token and the letter you output\"\"\"\n",
    "output = get_logits_after_thinking(prompt=prompt, max_new_tokens=9999)\n",
    "model.tokenizer.decode(output[1].view((-1)))\n",
    "model.tokenizer.decode(output[0].view((-1)).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer(\"A\"), model.tokenizer(\"B\"), model.tokenizer(\"C\"), model.tokenizer(\"D\"),\n",
    "[\"A\", \"B\", \"C\", \"D\"][output[0].softmax(-1).argmax()]\n",
    "# add a short sentence \"the answer is \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5586, 0.1289, 0.1260, 0.1855], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.tokenizer.decode(output[0].argmax())\n",
    "def get_answer_probabilities(logits, choices_count):\n",
    "    import torch.nn.functional as F\n",
    "    # Get probabilities for the first token after the prompt\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Map to token IDs for A, B, C, D, E\n",
    "    answer_tokens = [model.tokenizer.encode(chr(65+i))[0] for i in range(choices_count)]\n",
    "    \n",
    "    # Get probabilities for these tokens\n",
    "    answer_probs = [probs[token_id].item() for token_id in answer_tokens]\n",
    "    return answer_probs\n",
    "\n",
    "get_answer_probabilities(output[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer only with A, B, C or D. Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\nA) 0\\nB) 4\\nC) 2\\nD) 6\\nAnswer: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer only with A, B, C or D. Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "A) 0\n",
      "B) 4\n",
      "C) 2\n",
      "D) 6\n",
      "Answer: \n",
      "prompt from get_logits: prompt='Answer only with A, B, C or D. Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\nA) 0\\nB) 4\\nC) 2\\nD) 6\\nAnswer: '\n",
      "model.tokenizer.decode(input)='<｜begin▁of▁sentence｜><｜User｜>Answer only with A, B, C or D. Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\nA) 0\\nB) 4\\nC) 2\\nD) 6\\nAnswer: <｜Assistant｜><think>\\n'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Get model's logits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m logits = \u001b[43mget_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Get probabilities for each answer choice\u001b[39;00m\n\u001b[32m     49\u001b[39m probs = get_answer_probabilities(logits, \u001b[38;5;28mlen\u001b[39m(choices))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mget_logits\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28minput\u001b[39m = model.tokenizer.apply_chat_template(temp, tokenize=\u001b[38;5;28;01mTrue\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.tokenizer.decode(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m logits = out[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[39m, in \u001b[36mInterleavingTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m.invoker.\u001b[34m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m._model._envoy._reset()\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[39m, in \u001b[36mTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[32m     23\u001b[39m GlobalTracingContext.try_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/tracing/contexts/base.py:82\u001b[39m, in \u001b[36mContext.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     78\u001b[39m graph = graph.stack.pop()\n\u001b[32m     80\u001b[39m graph.alive = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/intervention/backends/remote.py:77\u001b[39m, in \u001b[36mRemoteBackend.__call__\u001b[39m\u001b[34m(self, graph)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph: Graph):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocking:\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# Do blocking request.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocking_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m \n\u001b[32m     81\u001b[39m         \u001b[38;5;66;03m# Otherwise we are getting the status / result of the existing job.\u001b[39;00m\n\u001b[32m     82\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.non_blocking_request(graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/intervention/backends/remote.py:294\u001b[39m, in \u001b[36mRemoteBackend.blocking_request\u001b[39m\u001b[34m(self, graph)\u001b[39m\n\u001b[32m    291\u001b[39m headers[\u001b[33m\"\u001b[39m\u001b[33msession_id\u001b[39m\u001b[33m\"\u001b[39m] = sio.sid\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# Submit request via\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m LocalContext.set(\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m *args: \u001b[38;5;28mself\u001b[39m.stream_send(*args, job_id=response.id, sio=sio)\n\u001b[32m    298\u001b[39m )\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;66;03m# Loop until\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/nnsight/intervention/backends/remote.py:185\u001b[39m, in \u001b[36mRemoteBackend.submit_request\u001b[39m\u001b[34m(self, data, headers)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresponse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResponseModel\n\u001b[32m    183\u001b[39m headers[\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mapplication/octet-stream\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/request\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    193\u001b[39m     response = ResponseModel(**response.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/reasoning_or_performing/venv/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/http/client.py:1411\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1409\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1413\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/http/client.py:324\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    326\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/http/client.py:285\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    287\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/ssl.py:1249\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1247\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1248\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dataset_name = \"cais/mmlu\"\n",
    "#! Use subjectyou know to understand reasoning\n",
    "dataset = load_dataset(dataset_name, \"abstract_algebra\", split=\"test\")  # Let's start with just 10 examples\n",
    "\n",
    "def format_mcq(question, choices):\n",
    "    # Format the question with choices as A, B, C, D, E\n",
    "    prompt = f\"Answer only with A, B, C or D. Question: {question}\\n\"\n",
    "    for i, choice in enumerate(choices):\n",
    "        prompt += f\"{chr(65+i)}) {choice}\\n\"\n",
    "    prompt += \"Answer: \"\n",
    "    display(prompt)\n",
    "    return prompt\n",
    "\n",
    "def get_answer_probabilities(logits, choices_count):\n",
    "    # logits should be of dimensions (num_tokens)\n",
    "    # Get probabilities for the first token after the prompt\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Map to token IDs for A, B, C, D, E\n",
    "    answer_tokens = [model.tokenizer.encode(chr(65+i))[0] for i in range(choices_count)]\n",
    "    \n",
    "    # Get probabilities for these tokens\n",
    "    answer_probs = [probs[token_id].item() for token_id in answer_tokens]\n",
    "    return answer_probs\n",
    "\n",
    "# Test the model on some examples\n",
    "correct = 0\n",
    "cnt = 0\n",
    "log = []\n",
    "for example in dataset:\n",
    "    if cnt >= 5:\n",
    "        break\n",
    "    cnt += 1\n",
    "    # display(example)\n",
    "    # assert False\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_answer = example['answer']  # A, B, C, D, or E\n",
    "    \n",
    "    # Format the question\n",
    "    prompt = format_mcq(question, choices)\n",
    "    \n",
    "    # Get model's logits\n",
    "    output = get_logits_after_thinking(prompt, max_new_tokens=9999)\n",
    "    logits = output[0]\n",
    "    log.append(dict(example=example, output=output))\n",
    "    \n",
    "    # Get probabilities for each answer choice\n",
    "    probs = get_answer_probabilities(logits, len(choices))\n",
    "    \n",
    "    # Get model's prediction\n",
    "    pred_idx = max(range(len(probs)), key=lambda i: probs[i])\n",
    "    pred_answer = chr(65 + pred_idx)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"Choices:\")\n",
    "    for i, choice in enumerate(choices):\n",
    "        print(f\"{chr(65+i)}) {choice}\")\n",
    "    print(f\"Correct answer: {correct_answer}\")\n",
    "    print(f\"Model predicted: {pred_answer}\")\n",
    "    print(f\"{pred_idx=}\")\n",
    "    print(f\"{pred_idx==correct_answer=}\")\n",
    "    print(f\"Answer probabilities: {dict(zip('ABCDE', probs))}\")\n",
    "    \n",
    "    if pred_answer == correct_answer:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"\\nAccuracy: {correct/len(dataset)*100:.2f}%\")\n",
    "#! Need to get reasoning length and find longest answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 22:52:04,933 98de260a-f4b5-45f2-b23e-9dabefb2be5f - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-10 22:52:05,391 98de260a-f4b5-45f2-b23e-9dabefb2be5f - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-10 22:52:05,576 98de260a-f4b5-45f2-b23e-9dabefb2be5f - RUNNING: Your job has started running.\n",
      "2025-03-10 22:52:07,226 98de260a-f4b5-45f2-b23e-9dabefb2be5f - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 6.47M/6.47M [00:00<00:00, 11.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output Logits:  tensor([[-1.4766,  0.9492, -0.9688,  ..., -0.3086, -0.3086, -0.3086],\n",
      "        [-1.4766,  0.9492, -0.9688,  ..., -0.3086, -0.3086, -0.3086],\n",
      "        [-1.4766,  0.9492, -0.9688,  ..., -0.3086, -0.3086, -0.3086],\n",
      "        ...,\n",
      "        [-1.4766,  0.9492, -0.9688,  ..., -0.3086, -0.3086, -0.3086],\n",
      "        [-1.4766,  0.9492, -0.9688,  ..., -0.3086, -0.3086, -0.3086],\n",
      "        [-1.4766,  0.9492, -0.9688,  ..., -0.3086, -0.3086, -0.3086]],\n",
      "       dtype=torch.bfloat16)\n",
      "Model Output:   Bounty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remote = True means the model will execute on NDIF's shared resources\n",
    "with model.trace(\"The Eiffel Tower is in the city of\", remote=True):\n",
    "\n",
    "    # user-defined code to access internal model components\n",
    "    model.model.layers[7].output[0][:] = 4 # in-place operation to change a single layer's output values\n",
    "    output = model.output.save()\n",
    "\n",
    "# after exiting the tracing context, we can access any values that were saved\n",
    "\n",
    "output_logits = output[\"logits\"]\n",
    "print(\"Model Output Logits: \",output_logits[0])\n",
    "\n",
    "# decode the final model output from output logits\n",
    "max_probs, tokens = output_logits[0].max(dim=-1)\n",
    "word = [model.tokenizer.decode(tokens.cpu()[-1])]\n",
    "print(\"Model Output: \", word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
